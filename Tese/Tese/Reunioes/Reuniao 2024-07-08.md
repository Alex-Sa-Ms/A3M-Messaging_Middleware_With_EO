# Conclusões
## Flag "throwIfNone"
- A flag `throwIfNone` quando definida num método, tem como objetivo a emissão de uma exceção na ausência de links estabelecidos e por estabelecer. Isto permite que peças (dinâmicas) da topologia, responsáveis por iniciar processos de estabelecimento de links e que nunca receberão tais pedidos, não fiquem eternamente bloqueadas. 
- Como os sockets são thread-safe, a situação de falta de links, poderia ser resolvida por uma outra thread, no entanto, por conveniência e para facilitar a programação, pensou-se no uso desta flag. 
- Este comportamento é invocado por uma flag já que uma peça fixa da topologia, como um servidor, aguarda que outras peças iniciem o processo de estabelecimento de link, sendo então desejável que este tipo de peças possa ficar bloqueada até que um link seja estabelecido e que não seja emitida uma exceção na ausência de links.
- A flag é primariamente empregada pelos métodos genéricos `waitForLink()` e `waitForAvailableLink()` que esperam pelo estabelecimento de um link ou pela disponibilidade para envio de um link estabelecido, respetivamente.
 - Esta flag pode ser utilizada também em métodos de envio e receção.
	 - O `AbstractSocket` pode ter uma implementação base dos métodos de envio e receção com o argumento `throwIfNone`. Estas implementações devem emitir a exceção `UnsupportedOperationException`. Se for decidido que se pretende implementar esse método, então faz-se o @Override e expõe-se esses métodos na classe *wrapper* do Socket.
## Link retries
### Solução 1
- Permitir criar sockets antes de iniciar a instância. Ao invocar `start()`, a instância passa a processar mensagens recebidas que incluem pedidos de ligação. Deste modo, evita-se rejeitar pedidos de ligação porque a criação do socket está atrasada.
	```
		// create middleware instance
		A3MMiddleware middleware = A3MMiddleware.new("node1","192.168.1.99", 12345); 
		// create socket
		RepSocket server = RepSocket.create("server", middleware);
		// starts middleware
		middleware.start();
	```
- O problema desta solução é não cobrir sockets que são criados após a instância ser iniciada.
### Solução 2 (final)
- Inclui a proposta da solução 1.
- Contempla retries de pedidos de ligação.
- O ZeroMQ não parece ter limites de pedidos de ligação, mas não custa incluir como uma opção base que tem como valor default "infinito". Possui, no entanto, um mínimo e máximo para o timeout entre cada pedido. A cada falha no pedido, o valor do timeout aumenta exponencialmente até ao valor máximo se este tiver sido definido. Se o valor máximo não tiver sido definido, o valor de timeout permanece igual.
	- Não deve ser necessário valores de timeouts aleatórios para proteger contra thundering herd, mas pode ser algo a mencionar na tese.
- As novas tentativas de ligação devem ser executadas pela única thread do middleware (a reader thread). Para isto é necessário que os sockets tenham um método privado, para não ser exposto às subclasses, que aquando da receção de uma resposta negativa a um pedido de ligação por ausência do socket destino, permite o socket criar um evento `LinkRetryEvent` e colocá-lo na queue de eventos a serem realizados pela reader thread.
	- Um evento do tipo `SendMsgEvent` que permitiria agendar o envio de uma mensagem parece uma solução válida e útil até para outras situações, no entanto, é importante que antes de reenviar o pedido de ligação se verifique que o socket não se encontra fechado e que o cliente não cancelou o processo de ligação com o destino em questão (através do método `unlink()`).
	- Para realizar estas verificações, a reader thread, no momento de execução do evento `LinkRetryEvent` apenas necessita de executar o método `linkRetry(sid : SocketIdentifier)` do socket que se encontra incluído no objeto do evento.
	- O método `linkRetry(sid : SocketIdentifier)` fará todas as verificações necessárias: (1) socket não se encontra fechado; (2) link em questão encontra-se num estado de aguardar retry.
	- Se o método tiver visibilidade "default" pode evitar-se que seja visto por classes que não devem, mas não deixa de ser poder ser exposto se uma subclasse do `AbstractSocket` for criada dentro do meu package.
## Expor eventos do socket
- A exposição de eventos do socket como "Link estabelecido com X", "Link recusado por X", "Sem links", etc.
- No caso de ser necessário e mesmo relevante a exposição destes eventos, estes podem ser expostos a partir de um método diferente do `receive()` de mensagens. Isto permite evitar que a programação do fluxo normal seja complicada desnecessariamente. 
- No fundo, isto poderia ser um *log* do socket.
- Neste momento, esta funcionalidade não aparenta ser necessária. ZeroMQ e NNG não possuem nada parecido com esta funcionalidade.

## Wait for link availability (to send)

Ensuring a link is available for sending a message is crucial for developing blocking send methods across various socket types. While some socket implementations can use a generic send method that includes the waiting functionality, others require a method that exclusively waits for sending availability, allowing the socket to decide if and what message should be sent.

Therefore, in addition to the generic blocking send method, two key functionalities are needed: waiting for a specific link to become available (`waitForLinkAvailability(sid: SocketIdentifier)`) and waiting for any link to become available (`waitForAvailableLink()`).

<!-- The "waiting" capability will be provided by a flow control coordinator, while the actual credits of each link will be managed by a flow control manager. This enables the separation of the concerns of coordinating threads and managing the flow control. In this section, we will be talking about different implementations of flow control coordinators and how a flow control coordinator is employed alongside a flow control manager. -->  

### Waiting for a Specific Link

Waiting for a specific link is a straightforward problem, commonly referred to as the Producer-Consumer problem. This can be easily solved using a single lock condition. When the link has credits, threads can return immediately. If the balance is insufficient, threads must wait using the lock condition. When credits are replenished, the lock condition is used to signal the appropriate number of threads that can be satisfied.

### Waiting for Any Link

The complexity increases when waiting for any link to become available. The main challenges are related to fairness, starvation and the thundering herd problem.

1. **Fairness/Starvation**: The primary issue is avoiding the prioritization of link-specific[^1] or general threads[^2].  Evenly distributing link credits between waiting threads and preventing threads from never being signaled depends on the fairness of the signaling mechanism chosen by the client.

2. **Thundering Herd**: This problem is addressed by avoiding signaling threads that cannot perform work, thereby preventing unnecessary context switches that degrade performance. Signaling a thread when the return condition cannot be met results in two unnecessary context switches since the thread needs to wait again.

[^1] Link-specific threads are threads interested in a single link.
[^2] General threads are threads interested in any link.

### Basic Implementation

A basic implementation might use a lock condition per link for link-specific waiting operations and a general lock condition for any link waiting operations. When a link's credits are replenished, the `signalAll()` method of both conditions could be invoked. Signaled threads would then check the link balance and return if is positive; if not, they would wait again. This approach, however, is not ideal due to potential races for credits, increased lock contention, pointless context switches, and starvation of slower threads.

### Advanced Implementations

The key to address the mentioned challenges is to use requests. There are two types of requests: link-specific and general. With each request corresponding to a thread's intention to wait for sending availability, the thundering herd problem is solved by only signaling the amount of threads that can be satisfied by the credit balance that each link allows. Regarding the starvation and fairness problems, two solutions are provided for the client to choose from: a *fair-ish* version and an unfair version. 

### Fair-ish implementation

To address the fairness and starvation problems, the devised solution involves requests that offer a sense of order. This order is achieved by giving to each request a ticket number. Each request is then placed in an appropriate queue depending on its type. Each link has its own queue of link-specific requests, and general requests are kept in a common queue. When the credits of a link are increased, it signals waiting threads based on the ticket numbers of the requests. The link peeks at the first ticket number of both its own link-specific requests queue and the general requests queue. The queue with the smallest ticket number (or with requests) dictates which type of waiting thread should be signaled. This approach solves the fairness/starvation problem between different types of threads. To ensure fairness among threads of the same type, a fair version of the lock is employed.

To prevent credits from being stolen, requests have three states: new, unsettled, and settled. A request is new when created, becomes unsettled when a thread is signaled, and becomes settled (deleted) when a thread reacquires the lock and secures the credit. The stealing action is effectively prevented by only allowing "waiting" threads to secure a credit when there are requests, of its type, that can be settled. New "waiting" threads can secure a credit immediately if there are enough link credits to cover both the new and unsettled requests, as well as the current request. While allowing this immediate action is not completely fair[^3], for efficiency purposes this solution seemed acceptable as it prevents two unnecessary context switches: one when threads enter the waiting queue; and another when they are signaled shortly after.

[^3] It is not entirely fair because a new thread is allowed to consume a credit before the queued threads.

<span style="color:red">Não esquecer de emitir exceção quando deixa de existir links. Semelhante ao processo feito para quando um link específico é eliminado.</span>
### Unfair Implementation

The unfair implementation is much more relaxed on the fairness and starvation problems. Apart from interleaving (when possible[^4]) signals of link-specific and general threads, there is no attempt in signaling threads in the order in which the requests were issued. Furthermore, credit stealing is not prevented. A thread that in the fair-ish version would be queued, due to the insufficient amount of credits to satisfy both the thread in question and the threads already waiting, can consume a credit instantaneously.

[^4] Interleaving is only possible when there are requests of both types.
<span style="color:red">Faz sentido fazer interleaving por link especifico ou faria mais sentido fazer interleaving a nível geral? Isto porque </span>
<span style="color:red">Usar algoritmo de sinalizacao em funcao do nº de threads a sinalizar? Na fair version nao da por causa da ordem.</span>
### Credit reservation

The issues surrounding "waiting for availability" are not over. 

The first problem is that a thread that has successfully returned from its "waiting" method invocation should be able to send a message to the link that became available whenever it chooses to. For this to be feasible, credits must be reserved. This reservation is achieved by preemptively consuming the credit, as the advanced implementations' do before the methods return. Without reserved credits, a thread that does not consume the credit immediately after returning from the waiting method[^5] might have its credit stolen by another thread[^6].

The second problem is related to the preemptive consumption of credits. A thread may decide not to send a message after waiting for availability. In such cases, a `releaseCredit()` method must be used to "release" the credit and to signal another thread. Releasing credits cannot be done simply by increasing the credits of the link, as this could lead to credit farming. To prevent farming, settled requests should be temporarily saved instead of being immediately deleted. A settled request should only be deleted when a thread either acquires or releases a credit. By making the `releaseCredit()` fail when there are no settled requests, credit farming is prevented. Credit acquisition, which acts more as a confirmation, is done by invoking `acquireCredit()`, which can also fail[^7] due to the lack of settled requests.

[^5] Until the socket lock is unlocked or the thread enters a waiting state using a lock condition.
[^6] Positive credits resulting from an increase in credit balance signal new threads. If a credit is not reserved, a new thread could be signaled, leading to a race for that credit. Additionally, a new "waiting" thread could acquire the credit if the necessary conditions are met.
[^7] The failure of the `acquireCredit()` method is not relevant, as the consumption of credits is done by the "wait...()" methods.

<p style="color:red">Usar um coordenador que nao pode atualizar diretamente os créditos pode resultar na falha de um envio pq creditos foram reservados no coordenador mas entretanto houve reducao de creditos que passou o nº de creditos efetivo do link para negativo ou mesmo 0. Com o coordenador desligado entao pode nao fazer sentido aceitar que as mensagens sejam enviadas já que os créditos efetivos nao foram alterados para manifestar as reservas dos creditos.  No entanto, se estiver ligado, a reserva é feita atraves do consumo do credito, indicando que a mensagem é como se tivesse sido efetivamente enviada, permitindo que as mensagens sejam enviadas imediatamente caso existam creditos por "adquirir".</p>

### Exposing the functionality

After discussing the implementation of the "waiting for availability" functionality, we will now address how this can be presented to developers who wish to implement a custom socket. The following methods are implemented by an abstract socket class as final protected methods:
- `waitForLinkAvailability(sid: SocketIdentifier)` and `waitForAvailableLink(): SocketIdentifier` are exposed as they are.
- `releaseSendCredit()` simply renames the `releaseCredit()` method.
- `sendDataMsg(message: Msg, reservedCredit: boolean)`, used to send messages. This method allows indicating if a credit has been reserved, thus enabling the waiting process to be skipped. The `acquireCredit()` method is not exposed directly since credit acquisition should only occur when a message is actually being sent.
- `sendDataMsg(message: Msg)`, which assumes that a credit has not been reserved, and thus, will block if necessary.
- `trySendDataMsg(message: Msg): boolean`, which attempts immediate sends. If the operation would normally block, the message is not sent, and "false" is returned. "True" is returned when the data message is successfully sent.

The send methods all include the word "data" to indicate that only data messages are limited by credits. Control messages bypass the flow control mechanism to avoid blocking their send, which could potentially lead to deadlocks.

Additionally, all methods that can block have variants allowing the specification of a timeout.

Custom sockets may further expose the waiting functionality to clients if it is deemed useful due to the socket's semantics.

<span style="color:red">Após pensar mais um pouco, separar o coordenador de espera dos créditos efetivos pode ser mesmo necessário. Uma thread que reserva vários créditos sobre um ou múltiplos links não deve poder enviar as mensagens quando lhe apetecer. Após reservar o crédito estou essencialmente a permitir que a thread demore o tempo que quiser e envie a mensagem sem qualquer restrição. Isto viola a possibilidade do recetor impedir o envio de mensagens já que existem envios "reservados", apesar de que num contexto exactly-once não devem existir restrições temporais.</span>
## Pollers

### Documentation
https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/?h=v6.10
https://lxr.linux.no/linux+v6.7.1/fs/select.c
https://man7.org/linux/man-pages/man2/select.2.html
https://man7.org/linux/man-pages/man2/poll.2.html
https://man7.org/linux/man-pages/man7/epoll.7.html
https://man.freebsd.org/cgi/man.cgi?kqueue

#### \[Important Kernel Concept\] "get" and "put" in Kernel Context
- **Reference Management**: Objects in the kernel often use reference counting to manage their lifetimes. Each time you acquire a reference to an object, you "get" it, which increments the reference count. When you're done with the object, you "put" it, which decrements the reference count.
- **Returning Object**: The "put" function effectively means "I'm done with this object; put it back into its pool or free it if no one else is using it." If the reference count drops to zero, indicating that no one else is using the object, it can be safely deallocated.
##### Practical Example

Consider a simplified reference counting system for an object:

```c
struct my_object {
    atomic_t refcount;
    // Other fields...
};

// Increment the reference count
struct my_object *my_object_get(struct my_object *obj) {
    atomic_inc(&obj->refcount);
    return obj;
}

// Decrement the reference count and free the object if it reaches zero
void my_object_put(struct my_object *obj) {
    if (atomic_dec_and_test(&obj->refcount)) {
        kfree(obj); // Free the object if the refcount reaches zero
    }
}
```
#### select()
##### Arguments
- A file descriptor set is passed for each type of event of interest: `readfds` (can read), `writefds` (can write), `exceptfds` (exceptional conditions). NULL can be passed if that type of event is not of interest.
- Another important argument is `nfds` which should be set to the highest file descriptor present in any of the three sets.  
- The file descriptor sets are interpreted as bit arrays, with each bit corresponding to a file descriptor. A bit as 1 conveys the interest in the event for that file descriptor.
- Each bit array (file descriptor set) needs to have a bit for each file descriptor until the highest mentioned file descriptor. For example, if the highest file descriptor we have interest in is 5705, then the sets need to have at least 5705 + 1 = 5706 bits (+1 because file descriptors start at 0). To prevent unaligned accesses, the size of the sets is actually calculated in terms of unsigned longs, i.e., the size should be a multiple of 64 bits. So the actual size would be 5760 bits = 90 unsigned longs.     
##### Underlying Data Structures
The underlying data structures used, which are also used by the poll() system call, are:
###### `struct poll_wqueues` 
- Structure: 
```
struct poll_wqueues {
	// initialized by "init_poll_funcptr(&pwq->pt, __pollwait)"
	// Holds the call that creates a wait queue entry and effectively
	// queues the current task to the be signaled by the file descriptors of interest.
	// Also holds the key, i.e., the events of interest
	poll_table pt; 

	// When the size of the inline poll table entries are not enough,
	// more space for poll table entries are allocated
	// in the format of poll table pages. 
	struct poll_table_page *table = NULL;

	// identification of the current task, which is required to wake
	// up the task when an event is ready
	struct task_struct *polling_task = current;   

	// set to 1 when a wait queue entry is processed
	int triggered = 0;  

	// if any error occurs, this is set
	int error = 0; 
	
	// index of the next inline entry to be created
	int inline_index = 0;  
	// faster access poll table entries
	struct poll_table_entry inline_entries[N_INLINE_POLL_ENTRIES]; 
};
```
- It is a structure used to hold all poll wait entries, is used to inform when a file descriptor of interest is ready (through the `triggered` variable) or when an error occurs, and also to hold the identification of the polling task (required to wake it up when an event is ready). 
- Holding the poll wait entries is required to enable wait entries to be deleted before `select()` returns.

###### `poll table` 
- Structure:
```
// Poll table
typedef struct poll_table_struct {
	poll_queue_proc _qproc; // function to queue the process in a poll queue
	__poll_t _key = ~(__poll_t)0; // events of interest. Initialized with all events enabled.
} poll_table;
```
- Essentially used to hold the function that queues the process when an immediate poll is not possible and also the key which indicates the events of interest.
- initialized by `init_poll_funcptr(&pwq->pt, __pollwait)`, which sets the call required to create wait queue entries and sets the key (all events are enable by default).
- The `_qproc` function is essential to queue the process in the queues of the file descriptors of interest, enabling the process to be signaled when an event of interest is ready.

###### `struct poll_table_entry`
- Structure:
```
struct poll_table_entry {
	struct file *filp;
	__poll_t key;
	wait_queue_entry_t wait;
	wait_queue_head_t *wait_address;
};
```
- This structures records the interest in a file descriptor. 
- The `filp` variable is a pointer to the information of the "file" of interest.
- The `key` variable informs the events of interest for that file.
- The `wait` variable is the pointer to the wait queue entry registered in the file's queue(s). (I think the file's have a queue for each type of event). This is required to identify to remove the entry, which was not processed, before the system call returns.
- The `wait_address` variable is the head of the queue where the wait queue entry was queued. This is required to delete entries before the select() system call returns.

###### `struct poll_table_page`
- Structure:
```
struct poll_table_page {
	struct poll_table_page * next;
	struct poll_table_entry * entry;
	struct poll_table_entry entries[];
};
```
- As mentioned before, this is nothing more than a structure used to allocate space for multiple poll table entries, when the inline entries array is already full and when the current page is full.
- 
###### `struct wait_queue_entry`
- Structure:
```
struct wait_queue_entry {
	unsigned int		flags;
	void			*private;
	wait_queue_func_t	func; // wake up function (pollwake)
	struct list_head	entry;
} wait_queue_entry_t;
```
- Wait queue entry which is used to register the interest of a process in waking up when an file is ready.
- The `private` variable holds the pointer to the `struct poll_wqueues` that defines the polling task, and is required to wake up the process.
- The `func` variable holds the wake up function which uses the `private` variable to do so.
###### `struct wait_queue_head`
- Structure:
```
struct wait_queue_head {
	spinlock_t		lock;
	struct list_head	head;
} wait_queue_head_t;
```
- Pointer to the entry that is the head of the queue. Required to keep track of the head of the queue, and allow entries to be removed when they are no longer required. An example, is the clean up of wait entries before the select() method returns. The clean up is required since each select() call is individual, i.e., is not related to any other select() call.
##### Algorithm
- The select() call iterates over all the file descriptors present in the file descriptor sets, and checks if there is any file ready for a specified event of interest.
- If a timeout of 0 is given, then creating wait entries is unnecessary, as they would be deleted right after.
- If a timeout higher than 0 is given, for each file descriptor which can block and has an event of interest, a wait entry is created.
- After iterating over all file descriptors, if a timeout was set and expired or a file descriptor is ready for an event of interest, the call exits the loop where the iteration through the file descriptors is done. It then proceeds to remove any existent wait entries, before returning.
- If after iterating over all file descriptors, there isn't a file descriptor ready and the timeout (if set) has not expired, the process sleeps until the timeout expires or it is signaled or a spurious awake happens. Upon waking up, it repeats the process of iterating over the file descriptors, and effectively exits the loop if the timeout expired. 
##### Notes
- select() can monitor only file descriptors numbers that
are less than FD_SETSIZE (1024)
#### poll()
- Very similar to select(). The difference lays in the structures used to pass the file descriptors and events of interest, as well as the structures used to iterate over them.
- Does not have a restriction regarding the file descriptors as the select() does.
- Instead of using different file descriptor sets for each of the events (read, write and exceptional), only a set of poll file descriptors are provided (the structure can be seen below). The poll file descriptors contain the file descriptor of an open file, the events of interest, and the variable to inform the events of interest which are ready.
- Structure of poll file descriptor:
```
struct pollfd {
	int fd;
	short events;
	short revents;
};
```
#### select()/poll() questions
1. **The "file->f_op->poll" function is the one responsible for creating and queuing the current task, right?**
    - Yes, the `file->f_op->poll` function is indeed responsible for checking the current status of the file descriptor and for queuing the current task if necessary. When `select()` or `poll()` is called, it will iterate over the file descriptors, invoking their `poll` method (via `file->f_op->poll`). This `poll` method typically calls functions like `poll_wait()` to add the current task to the wait queue associated with the file descriptor if it cannot immediately determine that the file descriptor is ready for the requested I/O operations.
    
2. **A socket has a wait queue for each type of event, or has a common queue for all events?**
    - A socket typically maintains separate wait queues for different types of events. For instance, there are distinct wait queues for reading, writing, and exceptional conditions. This allows the socket to wake up tasks waiting for a specific type of event without affecting tasks waiting for other types of events. This separation is beneficial because it provides finer granularity in event notification and improves efficiency.

3. **When an event is ready, the "file" iterates over a wait queue and wakes up the tasks that are interested in the file. When waking up, are the wait queue entries removed? Or, are the wait queue entries removed only before freeing the struct poll_wqueues?**
    - When an event is ready, the `wake_up` functions are used to wake up tasks waiting on the wait queue. Typically, when a task is woken up, its wait queue entry is removed. This is because the task no longer needs to wait for that event. However, in the context of `poll()` and `select()`, the wait queue entries are also removed when the polling operation completes, i.e., before the `poll_wqueues` structure is freed. This ensures that all wait queue entries associated with the polling operation are cleaned up properly to avoid dangling pointers or memory leaks.

In summary:

- `file->f_op->poll` handles the queuing of the current task.
- Sockets maintain separate wait queues for different event types.
- Wait queue entries are typically removed when the task is woken up, and they are also explicitly removed before freeing the `poll_wqueues` structure to ensure proper cleanup.
#### \[Only for code studying purposes, not for thesis\] Understanding select() / poll() underlying data structures


##### poll waiting queues
```
/*
 * Structures and helpers for select/poll syscall
 */
struct poll_wqueues {
	poll_table pt; // initialized by "init_poll_funcptr(&pwq->pt, __pollwait)"
	struct poll_table_page *table = NULL;
	struct task_struct *polling_task = current;
	int triggered = 0;
	int error = 0;
	int inline_index = 0;
	struct poll_table_entry inline_entries[N_INLINE_POLL_ENTRIES];
};
```

##### poll table
```
// Poll table
typedef struct poll_table_struct {
	poll_queue_proc _qproc = *1; // function to queue the process in a poll queue
	__poll_t _key = ~(__poll_t)0; // events of interest. Initialized with all events enabled.
} poll_table;
```

```
------------------>  *1  <------------------

// _qproc initialized value
static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
				poll_table *p)
{
	// calculates pointer of struct poll_wqueues which owns the poll_table
	struct poll_wqueues *pwq = container_of(p, struct poll_wqueues, pt);
	
	// gets pointer to next poll table entry. Creates a table page if required.
	struct poll_table_entry *entry = poll_get_entry(pwq);
	if (!entry)
		return;
	
	// fills entry with file descriptor, wait address and key (events enabled)	
	entry->filp = get_file(filp);
	entry->wait_address = wait_address;
	entry->key = p->_key;

	// initializes wait queue entry with poll wake up function
	init_waitqueue_func_entry(&entry->wait, pollwake);

	// ?? Ainda nao sei para que é preciso o identificador da pwq
	entry->wait.private = pwq;
	
	// adds wait queue entry to the queue
	add_wait_queue(wait_address, &entry->wait);
}
```

```
// format of a function to queue a process on poll queue? 
typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *, struct poll_table_struct *);

// In this function, we can see that the poll_table, "p", adds itself to the poll queue 
static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)
{
	if (p && p->_qproc && wait_address)
		p->_qproc(filp, wait_address, p);
}
```

##### poll table page

```
struct poll_table_page {
	struct poll_table_page * next;
	struct poll_table_entry * entry;
	struct poll_table_entry entries[];
};
```
##### poll table entry
```

struct poll_table_entry {
	struct file *filp;
	__poll_t key;
	wait_queue_entry_t wait;
	wait_queue_head_t *wait_address;
};
```

##### wait queue entry
```
struct wait_queue_entry {
	unsigned int		flags;
	void			*private;
	wait_queue_func_t	func; // wake up function (pollwake)
	struct list_head	entry;
} wait_queue_entry_t;
```

```
struct wait_queue_head {
	spinlock_t		lock;
	struct list_head	head;
} wait_queue_head_t;
```

```
static int pollwake(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
{
	struct poll_table_entry *entry;

	// gets poll table entry that owns the wait queue entry
	entry = container_of(wait, struct poll_table_entry, wait);
	
	//
	if (key && !(key_to_poll(key) & entry->key))
		return 0;
	return __pollwake(wait, mode, sync, key);
}
```

#### epoll()
##### structures
###### struct epoll_filefd
- Used in "struct epitem" to identify the file.
- Structure:
```
struct epoll_filefd {
	struct file *file;
	int fd;
} __packed;
```
###### struct  eppoll_entry
- The `eppoll_entry` structure is used to manage individual events and their associated file descriptors in the `epoll` system. It tracks the state of each file descriptor being monitored, handles wait queues for events, and facilitates efficient event notification.
- `next` ...
- `base` ...
- `wait` ...
- `whead` ...
```
/* Wait structure used by the poll hooks */
struct eppoll_entry {
	/* List header used to link this structure to the "struct epitem" */
	struct eppoll_entry *next;

	/* The "base" pointer is set to the container "struct epitem" */
	struct epitem *base;

	/*
	 * Wait queue item that will be linked to the target file wait
	 * queue head.
	 */
	wait_queue_entry_t wait;

	/* The wait queue head that linked the "wait" wait queue item */
	wait_queue_head_t *whead;
};
```
###### struct epitem
- 
```
/*
 * Each file descriptor added to the eventpoll interface will
 * have an entry of this type linked to the "rbr" RB tree.
 * Avoid increasing the size of this struct, there can be many thousands
 * of these on a server and we do not want this to take another cache line.
 */
struct epitem {
	union {
		/* RB tree node links this structure to the eventpoll RB tree */
		struct rb_node rbn;
		/* Used to free the struct epitem */
		struct rcu_head rcu;
	};

	/* List header used to link this structure to the eventpoll ready list */
	struct list_head rdllink;

	/*
	 * Works together "struct eventpoll"->ovflist in keeping the
	 * single linked chain of items.
	 */
	struct epitem *next;

	/* The file descriptor information this item refers to */
	struct epoll_filefd ffd;

	/*
	 * Protected by file->f_lock, true for to-be-released epitem already
	 * removed from the "struct file" items list; together with
	 * eventpoll->refcount orchestrates "struct eventpoll" disposal
	 */
	bool dying;

	/* List containing poll wait queues */
	struct eppoll_entry *pwqlist;

	/* The "container" of this item */
	struct eventpoll *ep;

	/* List header used to link this item to the "struct file" items list */
	struct hlist_node fllink;

	/* wakeup_source used when EPOLLWAKEUP is set */
	struct wakeup_source __rcu *ws;

	/* The structure that describe the interested events and the source fd */
	struct epoll_event event;
};
```
###### struct eventpoll
- epoll_create() returns a file descriptor, which means that epoll is created as a "file". This structure corresponds to the private data specific to this type of "file" called epoll.
- Structure:
```
/*
 * This structure is stored inside the "private_data" member of the file
 * structure and represents the main data structure for the eventpoll
 * interface.
 */
struct eventpoll {
	/*
	 * This mutex is used to ensure that files are not removed
	 * while epoll is using them. This is held during the event
	 * collection loop, the file cleanup path, the epoll file exit
	 * code and the ctl operations.
	 */
	struct mutex mtx;

	/* Wait queue used by sys_epoll_wait() */
	wait_queue_head_t wq;

	/* Wait queue used by file->poll() */
	wait_queue_head_t poll_wait;

	/* List of ready file descriptors */
	struct list_head rdllist;

	/* Lock which protects rdllist and ovflist */
	rwlock_t lock;

	/* RB tree root used to store monitored fd structs */
	struct rb_root_cached rbr;

	/*
	 * This is a single linked list that chains all the "struct epitem" that
	 * happened while transferring ready events to userspace w/out
	 * holding ->lock.
	 */
	struct epitem *ovflist;

	/* wakeup_source used when ep_send_events or __ep_eventpoll_poll is running */
	struct wakeup_source *ws;

	/* The user that created the eventpoll descriptor */
	struct user_struct *user;

	struct file *file;

	/* used to optimize loop detection check */
	u64 gen;
	struct hlist_head refs;

	/*
	 * usage count, used together with epitem->dying to
	 * orchestrate the disposal of this struct
	 */
	refcount_t refcount;

#ifdef CONFIG_NET_RX_BUSY_POLL
	/* used to track busy poll napi_id */
	unsigned int napi_id;
	/* busy poll timeout */
	u32 busy_poll_usecs;
	/* busy poll packet budget */
	u16 busy_poll_budget;
	bool prefer_busy_poll;
#endif

#ifdef CONFIG_DEBUG_LOCK_ALLOC
	/* tracks wakeup nests for lockdep validation */
	u8 nests;
#endif
};
```

###### struct ep_pqueue
```
/* Wrapper struct used by poll queueing */
struct ep_pqueue {
	poll_table pt;
	struct epitem *epi;
};
```

###### struct epitems_head
```
/*
 * List of files with newly added links, where we may need to limit the number
 * of emanating paths. Protected by the epnested_mutex.
 */
struct epitems_head {
	struct hlist_head epitems;
	struct epitems_head *next;
};
```
###### struct  epoll_event
- Used to inform which events are of interest.
- Structure:
```
struct epoll_event {
	__poll_t events;
	__u64 data;
} EPOLL_PACKED;
```
##### epoll_create()
- Creates and initializes an epoll instance. It also creates a file instance for the epoll instance, and associates both of them.
##### epoll_ctl()
- Enables the insertion/removal/change of file descriptors inside the interest set.
- Process:
	1. Checks if the file descriptor in question is pollable before doing any operation.
	2. Does not allow adding the epoll instance inside itself.
	3. Setting poll exclusivity is only allowed when adding an event of interest since adding to the waiting queue is only performed at adding time. Also, it is not possible to add exclusivity if the file of interest is an epoll instance or if flags incompatible with EPOLLEXCLUSIVE flag are set. 
		- The flags compatible with the EPOLLEXCLUSIVE flag are:
			- EPOLLIN | EPOLLOUT | EPOLLERR | EPOLLHUP | EPOLLWAKEUP | EPOLLET | EPOLLEXCLUSIVE
	4. Acquire the epoll instance mutex (uses nonblock flag to determine if it should be an attempt or if it should commit to the action, i.e., if it should block)
	5. If the operation in question is to add a file of interest:
		1. Checks for potential epoll instance loops and deep chains that surpass the length limit.
		2. An error is thrown if a loop is found or if the new insertion would result in a chain deeper than the allowed limit.
	6. Attempts to get the "struct epitem" associated with the file in question. If this exists, it is present in the RB tree of the epoll instance.
	7. Now, the specific process for each type of operations is done:
		1. If the operation is to "add" a new file descriptor:
			1. If a "struct epitem" was found, then return an error informing that the file is already being monitored.
			2. Else, add to the events of interest the flags regarding errors (EPOLLERR) and hang ups (EPOLLHUP), and perform the insertion using `ep_insert()`.
				- `ep_insert(struct eventpoll *ep, const struct epoll_event *event, struct file *tfile, int fd, int full_check)`:  
					1. Checks if the target file is an epoll instance. If it is gets the private data that allows accessing the epoll instance data. 
					2. Initializes a "struct epitem": 
						- Initializes the `rdllink` struct (used to link the epitem with eventpoll ready list, `ep->rdllist`), sets the epoll instance, the target file descriptor, the events and marks the "next" epitem as an unactive pointer.
					3. If the target file is an epoll instance, acquires the target epoll instance mutex.
					4. Attaches the epitem to the list of epoll hooks of the file through the function `attach_epitem(tfile, epi)`.
						- `attach_epitem()`:
							1. If the target file is an epoll instance, the epitem will be added to the `ep->refs` list, which keeps the epoll instances that have the target epoll instance reference (The `ep->refs` is made to match the `file->f_ep` if the target file is an epoll instance). If the target file is not an epoll instance, a list (`file->f_ep`) is allocated for the target file to keep the epoll instances that monitor it.
							2. Acquires the file lock.
							3. If the file does not have a list of epoll instances (`file->f_ep`) sets the allocated list as the file's list of epoll instances. Otherwise, frees the allocated list.
							4. Adds the epitem to the target file's list of epoll instances that have its reference, `file->f_ep`, which is the same as `ep->refs` if the target file is an epoll instance. This insertion is made by adding epitem's fllink (`epi->fllink`) node to the head of the list.
					5. If a full check has to be performed and the target file is not an epoll instance, then add the file to the list of target files to be checked (`tfile_check_list`).
					6. Adds the epitem to the epoll instance RB tree, and unlocks the mutex if the target file is an epoll instance.
					7. Checks if too many backpaths have been created through  a reverse path check that uses the `tfile_check_list`. This list contains links proposed to be newly added.
					8. Removes the epitem safely if too many backpaths were created.
					9. Handles the EPOLLWAKEUP flag if present.
					10. Initializes the poll table for the epitem using the queue callback `ep_ptable_queue_proc`.
						- `ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead, poll_table *pt)`:
							- This function is used to add the wait queue to the target file wakeup lists.
							1. Allocates a "struct eppoll_entry" `pwq`.
							2. Initializes the wait queue entry, `pwq->wait`, with the `ep_poll_callback`.
							3. Sets the `pwq` reference to the wait queue head and the associated epitem.
							4. If the `EPOLLEXCLUSIVE` flag is set, adds the wait queue entry, with a flag indicating exclusivity, to the wait queue list of the file. Exclusive wait queue entries are added at the tail. If the `EPOLLEXCLUSIVE` flag is not set, adds the wait queue entry to the head of the wait queue list.
								- During wake-up, all regular entries are handled and only one exclusive entry is handled.
							5. Adds the eppoll entry, `pwq`, to the head of the epitem's list of eppoll entries.
								- Each file may have multiple queues, generally, one per event type (POLLIN, POLLOUT, POLLERR).
						- `ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)`
							- This is the callback that is passed to the wait queue wakeup mechanism. It is called by the stored file descriptors when they have events to report.
							1. First extracts the epitem from the eppoll entry associated with the wait queue entry.
							2. Checks if the epitem's event mask has any poll events. If it doesn't the descriptor is considered to be disabled.  Then, Jumps to step .... Not having events is most likely the result of EPOLLONESHOT disabling the events until the next EPOLL_CTL_MOD.
							3. Checks if the poll flags (events reported by the device) match any of the events of interest. If there isn't a match, jumps to ....
							4. Due to not being allowed to hold a lock when transferring events to userspace, if the overflow list of the epoll instance is not unactive (`ep->ovflist != EP_UNACTIVE_PTR`), then the epitem must be stored in the tail of the overflow list so that it can be requeued later on. If the overflow list is unactive, then the link between the epitem and the epoll instance ready list is checked. If the epitem is not linked, then it is added to the tail of the ready list (making it linked).
							5. Having the epitem added to the ready list, wake up of the eventpoll wait list and the ->poll() wait list follows. 
							6. If the POLL_FREE is set, removes the wait entry from the wait entry queue.
					11.  Attaches the item to the poll hooks and gets current event bits, i.e., performs the poll operation on the target file. 
						1. First, updates the poll table to manifest the events of interest.
						2. If the target file is not an epoll instance, performs the `vfs_poll()`, which results in the attachment of the epitem to the wait queue(s) of the target file if none of the events of interest is available. If the target file is an epoll instance performs `__ep_eventpoll_poll()` which starts by inserting the wait queue entry in the target epoll instance's poll_wait queue and then iterates over the entries in its ready list and overflow list to determine if the wanted events are effectively available. If there are available events, returns the IN event.
						3. Returns the events of interest that are available.
					12. If there was any error during the install process of the poll wait queue, namely lack of memory, then remove the epitem safely from the epoll instance, and return the error.
					13. Finally, if the file is already ready, add the epitem to the tail of epoll instance ready list and notify waiting tasks both in `ep->wq` and `ep->poll_wait`.
		2. If the operation is to "delete" a file descriptor:
			1. If the epitem does not exist,  returns error informing that no such entry exists.
			2. Else, safely removes it using `ep_remove_safe(ep, epi)`, which is equivalent to `__ep_remove(ep, epi, false)`.
				- `static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)`
					- Removes a "struct epitem" from the eventpoll RB tree and deallocates all the associated resources. If the dying flag is set, do the removal only if force is true.
					1. Starts by removing the poll wait queue hooks. This is done by iterating over the entries ("struct eppoll_entry") inside the epoll instance pwqlist.
					2. Now, its time to remove the epitem from the epoll hook list of the target file. Apart from removing the item, if the item is the only item in the file's list and the file is not an epoll instance, the list is freed.
					3. Removal of the epitem from the epoll instance RB tree.
					4. If the epitem is linked to the ready list of the epoll instance, removes the link.
					5. Finally, frees the epitem.
		3. If the operation is to "modify" a file descriptor:
			1. If the epitem does not exist, returns an error informing that no such entry exists.
			2. Else, applies the modifications to the file descriptor using `ep_modify()` if the EPOLLEXCLUSIVE flag has not been set previously. If this flag was set previously, then no modification can be performed.
				- ``static int ep_modify(struct eventpoll *ep, struct epitem *epi, const struct epoll_event *event)`
					- Modify the interest event mask by dropping an event if the new mask has a match in the current file status.
					1. Updates the "struct epoll_event" of the existing epitem to manifest the new events of interest.
					2. Then polls using the updated epitem to get the current event bits. If the epitem is "hot" and is not registered in the epoll instance ready list, push it inside and notify the waiting tasks.
			- Note: *A new poll table is created without a process queuing callback is created to perform the poll. Not having such callback means not creating wait queue entries. This seems okay since the file descriptor is being modified and there should already be a wait queue entry. However, if devices have multiple wait queue entries and if I added the interest in the EPOLLOUT event, how will the a waiting queue entry for the EPOLLOUT event be registered?*
##### epoll_wait
- `int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);`
- Calls `static int do_epoll_wait(int epfd, struct epoll_event __user *events, int maxevents, struct timespec64 *to)`:
	1. Checks if number of max events is valid, if user's area for events is writable and if the epoll instance file descriptor is valid.
	2. Then, fish for events using `ep_poll(ep, events, maxevents, to`
		- `static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, struct timespec64 *timeout)`
			- Retrieves ready events, and delivers them to the caller-supplied event buffer.
			1. Checks timeout and sets the timed_out flag is the user specified an non blocking operation.
			2. Then, checks if there are available events (i.e. if at least one of events lists (ready list or overflow list) is not empty).
			3. Since there are timeouts, the rest of the function is kept inside a loop so that it the operations can be repeated until events are ready or the timeout expires.
			4. Loop:
				1. If there are events available try to send them to the user space using `ep_send_events(ep, events, maxevents)` and return the amount of events transferred if successful.
					- `static int ep_send_events(struct eventpoll *ep, struct epoll_event __user *events, int maxevents)`
						1. Checks for pending interrupt signal to allow threads to exit and prevent them from fetching events repeatedly.
						2. Creates a poll table that will be used to poll all files and verify that events are indeed ready.
						3. Moves the items in the ready list to temporary list and makes the overflow list writable for concurrent events to not be lost.
						4. For every epitem on the temporary list:
							1. Removes it from temporary list through the `epi->rdlink` variable;
							2. Polls using the epitem to verify the existence of available events.
							3. If there aren't available events, skip the rest of the actions for this epitem. Else, performs the rest of the actions that follow.
							4. Copies the event on the caller's event list.
							5. If the copy fails, adds the event back to the temporary list using the `epi->rdlink` variable, sets the return value to a segmentation fault error and exits the loop.
							6. If the EPOLLONESHOT flag was set, performs `epi->event.events &= EP_PRIVATE_BITS;` with EP_PRIVATE_BITS = (EPOLLWAKEUP | EPOLLONESHOT | EPOLLET | EPOLLEXCLUSIVE). This essentially, removes flags and prevents the wake up of tasks using this epitem while the flags are not reset using EPOLL_CTL_MOD.
							7. If both EPOLLONESHOT and EPOLLET flags were not set, i.e., if the file is being monitored in Level-Triggered mode, therefore the event is added back inside the ready list.
						5. After the iterations or after a segmentation fault that resulted in the loop terminating early, the epoll instance ready list must be updated. Entries in the temporary list that were not processed due to a segmentation fault that ended the loop early, and entries present in the overflow list are moved to the ready list. The items are added so that the order in which the events were emitted is kept. If the ready list, in the end, is not empty, tasks waiting on `ep->wq` are notified.
				2. If there are no events available and the timeout is over, return 0, indicating that no event was ready. 
				3. If a busy loop was asked, skips the rest of the iteration, i.e., goes back to step 1.
				4. Checks if an interrupt signal is pending. If affirmative returns an error indicating such situation.
				5. Creates a wait queue entry with `ep_autoremove_wake_function`.
					- The  `ep_autoremove_wake_function` calls the default wake function (which uses the wake function for a thread task) and deletes the wait entry from the `ep->wq` wait queue.
				6. If events are not available adds the wait entry as an exclusive entry (so that only one entry is woken up at a time), and sleeps until timeout or until being woken up.
				7. If woken up, try to harvest some events.
				8. If the task woke up due to the timeout and its wait entry is still present in the wait queue, then removes the wait entry and returns without trying to get some events. If it timed out and was woken up concurrently, then it it can try to harvest some events before returning.

##### ep_eventpoll_release
- Called when the file descriptor associated with the epoll instance is released, i.e., when the system call `close()` is called with the epoll instance file descriptor.
- Corresponds to calling the function `ep_clear_and_put()`
	- `static void ep_clear_and_put(struct eventpoll *ep)`
		- 

##### Notes
###### Poll hook lists vs Epoll hook lists
The *poll hook lists*, i.e., the wait queue lists are the lists used by devices to communicate when an event is ready. Epoll instance items (epitems) are associated with wait queue entries present in these lists which indicates that the epoll instances are notified via poll hooks. The epoll hook lists (file->f_ep), as far as my understanding goes, seems to be used solely for the purpose of preventing loops between epoll instances and deep chains.

###### Study how wake up source is used
- Check the `struct wakeup_source`. Can it be similar to the unfair credit wake up?

###### Check ep_poll handling of timeouts
- The `ep_poll()` function handles the timeouts elegantly.
#### \[Only for code studying purposes, not for thesis\] Understanding epoll
##### File callbacks
```
/* File callbacks that implement the eventpoll file behaviour */
static const struct file_operations eventpoll_fops = {
#ifdef CONFIG_PROC_FS
	.show_fdinfo	= ep_show_fdinfo,
#endif
	.release	= ep_eventpoll_release,
	.poll		= ep_eventpoll_poll,
	.llseek		= noop_llseek,
	.unlocked_ioctl	= ep_eventpoll_ioctl,
	.compat_ioctl   = compat_ptr_ioctl,
};
```


#### kqueue()




### Ideas & Notes
- Allowing events, exceptions to be subscribed can also be beneficial.
- Poller nesting should not be provided for now.
- Use caches for frequently created objects like Epitem and WaitQueueEntry? Has initial overhead but then its supposed to be faster.
### Problems
#### How can it work in conjunction with the "waiting for availability" mechanism?
<span style="color:red">TO-DO</span>
#### Level-triggered, edge-triggered, mixed, custom?
- Like epoll(). LT or ET.
#### Return one or all available?
- Like epoll(), it returns every pollable that is in the ready list (and that after confirmation has events available).
#### Thread-safe or not?
- Yes, a poller instance can be used by multiple threads simultaneously. 
- LT events can reach every thread. They may not reach all waiting threads if the pollable ceases to be available before all waiting threads are woken up.
- ET events only reach one thread.
#### Credit Reservation & Fairness model
- The purpose of credit reservation was to prevent threads from stealing credits from each other. Even though in a fair scenario, having credit reservation is ideal, making this a requirement means forcing every pollable to implement a credit mechanism which is not desirable.
- The fairness model used will be fairish, i.e. not completely fair. Even though the actual implementation depends on the object's semantics, the operations should consider the existing waiters. For example, if a socket has 2 waiters waiting to send a message and there are only 2 credits to send messages, if a 3rd thread attempts to send a message it should be added to the waiters queue instead of sending the message and consuming a credit.
	- The modifying operations, such as sending and receiving, unlike polling operations, must be registered in the wait queue as EXCLUSIVE. This is important to avoid "stealing" as much as possible. 
- Instead of reserving credits, we must make the client aware that the application must be designed having in mind that in a multi-threaded context an event said to be available by a poller or even a wait() function of the object of interest may become unavailable. Having that said, the client should opt for non-blocking methods when using mechanisms that do not act on the event immediately as receive() and send() methods do.
	- Let's consider an example. Let's consider a socket X, that currently does not have any messages to be received, and a thread A, which polled socket X in order to know when it is possible to receive a message. Now, let's assume that between the moment of awakening of thread A and its invocation of the method receive() on the socket X, a thread B decides to invoke the said receive() method on socket X. Thread B will "steal" the message from thread A as it does not observe any waiting threads in queue. With this example, we can know understand why the use of non-blocking methods are recommended after being notified by a poll mechanism.

#### How do select() and poll() know which objects are ready, and how does the kernel know which process to wake up.
- After waking up, they iterate over all the file descriptors of interest.
#### Can the "wait for availability coordinator" be somehow used to implement a thread-safe poller? Each socket would essentially have a send poller with all links.
<span style="color:red">TO-DO: Wait for availability will probably cease to exist.</span>
#### Poll method and incompatible requested events
- What should happen when a client subscribes to events that are not supported by the object of interest?
	- Nothing. A pollable object only reports (wakes up waiters) when events of interest happen. 
	- For instance, if a client attempts to poll a read event from a PUSH socket, it will be stuck forever unless a timeout is provided. PUSH sockets push messages, i.e., they only send messages. Since a data message will never be received, there won't be an event that wakes up the client.
#### Using EXCLUSIVE polling
- `POLLEXCLUSIVE` polling must be used with care. A client that polls using the `POLLEXCLUSIVE` flag must handle the available events, this is because when `POLLEXCLUSIVE` is used, only one thread `POLLEXCLUSIVE` thread is woken up. If there aren't non `POLLEXCLUSIVE` threads to handle the events in the place of the `POLLEXCLUSIVE` thread, another `POLLEXCLUSIVE` waiter will not be woken up until a new event occurs. The problem is that a new event may not occur until the current events are dealt with, therefore, the waiters will hang forever.
- Like in epoll(), the `POLLEXCLUSIVE` flag can only be set when adding a pollable object to the poller.

#### EXCLUSIVE polling and static wait entry position
- A waiter with `EPOLLEXCLUSIVE` flag does not change position when it is woken up, and since the most common wake up only notifies the first exclusive wait entry, the same wait queue entry will be notified. How can this be solved?

- **Idea 1 (Chosen because of simplicity):**
	- Relocate exclusive entries to the tail. 
	- *Pros:*
		- Does not require additional memory resources.
		- Wait queue does not need to be changed as the desired behavior can be achieved when required by performing the relocation of the entry to the tail of the wait queue.
	- *Cons:*
		-  Its considerably less efficient in terms of computing resources with O(5) complexity for relocating each exclusive wait entry
			- `current->prev->next = current->next;`
			- `current->next->prev = current->prev;`
			- `current->prev = tail;`
			- `current->next = null;`
			- `tail->next = current;`

- **Idea 2:**
	- Having a pointer to last exclusive wait entry notified. If the last exclusive entry notified does not have a following entry, the wake up loop can find the exclusive entry that should be notified after the non-exclusive entries.
	- *Pros*:
		- May efficient regarding computing resources. Only requires updating a single variable.
	- *Cons:*
		- Remove logic is a bit more complex. It must update the "last notified" variable if the "last modified" corresponds to the entry being removed.
		- Wake up logic is more complex. The wake up loop must stop after waking up all non-exclusive entries. The process of waking up exclusive entries starts at the entry that follows the "last notified" entry, therefore, the last non-exclusive entry must be tracked enabling to know where exclusive entries start in case the tail of the wait queue is reached and the number of exclusive entries to be woken up is still positive. When the number of exclusive entries to be woken up is higher than 1, the "last notified" entry before waking up any exclusive entries should be tracked to prevent waking up the same exclusive entries multiple times.
		- "last notified" variable may be useless depending on the wanted semantics.
		- ***Considering that exclusive wait queue entries may be removed or added, having a head for exclusive entries is necessary.***
	- Example (not correct as it is missing the point above) of how the wake up function would be:
```c
static int __wake_up_exclusive_fairness(struct wait_queue_head *wq_head, unsigned int mode,
			int nr_exclusive, int wake_flags, void *key)
{
	wait_queue_entry_t *curr, *next; 
	struct list_head *last_non_excl, // last non exclusive 
					 *first_excl = NULL,
					 *it_excl; // first exclusive

	lockdep_assert_held(&wq_head->lock);

	curr = list_first_entry(&wq_head->head, wait_queue_entry_t, entry);

	if (&curr->entry == &wq_head->head)
		return nr_exclusive;

	list_for_each_entry_safe_from(curr, next, &wq_head->head, entry) {
		unsigned flags = curr->flags;
		int ret;
		
		// stops after reaching exclusive entries
		if (flags & WQ_FLAG_EXCLUSIVE){
			// tracks the last non-exclusive entry as it may be required
			last_non_excl = container_of(curr->entry->prev, struct wait_queue_entry_t, entry);
	
			// sets the first exclusive entry to be woken up
			if(wq_head->last_notified)
				first_excl = wq_head->last_notified->next;
			if(!first_excl)
				if (last_non_excl)
					first_excl = last_non_excl->next;		
				else
					first_excl = wq_head->head; // if there aren't non-exclusive entries, the exclusive entries start at the head.
			break;
		}
		
		ret = curr->func(curr, mode, wake_flags, key);
		if (ret < 0)
			return nr_exclusive;	
	}

	// if there are exclusive entries, iterates over them
	if(first_excl){
		it_excl = first_excl;
		bool loop = true;
		while(loop){
			list_for_each_entry_safe_from(curr, next, it_excl, entry) {
				// stops if reaching an exclusive entry for the second time
				if(curr->entry == first_excl){
					loop = false;
					break;
				}

				unsigned flags = curr->flags;
				int ret;

				ret = curr->func(curr, mode, wake_flags, key);
				if (ret < 0)
					break;

				if (ret && !--nr_exclusive){
					// sets last notified
					wq_head->last_notified = curr->entry;
					loop = false;
					break;
				}
			}

			// enables the loop to restart from 
			if(last_non_excl)
				it_excl = last_non_excl->next;
			else
				it_excl = wq_head->head;
		}
	}

	return nr_exclusive;
}
```

- **Idea 3:**
	- Having separate queues, one regular queue (list) for non-exclusive entries, and a circular queue for exclusive entries. 
	- A "last exclusive entry notified" variable or a "next exclusive entry to notify" variable is required, as well as a counter of existing exclusive entries.
	- *Pros:*
		- 
	- *Cons:*



#### Pollables
- Pollables must have a non-null ID that is locally unique.
- The ID facilitates the registration of pollables in the poller, since only a single data structure for quick look-ups is required. 
- The pollables will be stored using the hash code of the ID. 
- Currently, the different pollables may have objects of different classes as IDs. ![[Pasted image 20240807141913.png]]
	- In the future, it would be wise to centralize and generalize the IDs in order to minimize collisions. For instance, each pollable could receive an integer ID at creation time. 
	- To ensure correctness, the pollables would probably need to be created as final objects, instead, of being arbitrary objects as allowed by the pollable interface. ![[Pasted image 20240807141308.png]]
		- The release() method sets the identifier and the poll function of the instance to null, therefore, making the pollable unusable. An attempt of using a released pollable must result in an exception.

#### Events
- The events that can be monitored are: `POLLIN` (read), `POLLOUT` (write), `POLLERR` (error) and `POLLHUP` (hang up). The monitoring of hang up event is mandatory as it informs when a pollable is closed. 
- Similar to epoll(), level-triggering (`POLLLT`) and edge-triggering (`POLLET`) is supported.
- Regarding edge-triggering, it is possible to set `POLLEXCLUSIVE` and/or `POLLONESHOT` flags. 
	- The `POLLEXCLUSIVE` flag is used in a way that only one waiter is woken up when an event is available for the object of interest. For example, if the same pollable is registered in two different pollers using the combination of flags, `POLLET` and `POLLEXCLUSIVE`, and an event occurs, only one of the pollers is woken up.
	- The `POLLONESHOT` is useful when draining the pollable of interest is not desired. By setting this flag, after being notified, the epoll instance removes the interest in the events of the pollable. The pollable will remain in the interest list, however, events will not be notified until the re-registration of the events of interest using the modify() method. 

#### Poller API
- Similar to epoll().
- The wait() method of the poller returns a list of pairs of the id of the pollable and bit mask containing the available events.

#### What to do when an error occurs?
- Depending on the error's seriousness the pollable should decide on whether a common wake up should be performed or if the exclusivity should be ignored therefore resulting in waking up all waiters. When the waiters perform the poll after being woken up, they should get an EPOLLERR flag.
#### What to do when the pollable is closed?
- The pollable must wake up all waiters regardless of the exclusivity.
- The pollable's waiters will receive the EPOLLHUP flag when performing the poll after waking up. 
	- The EPOLLERR must be set as well, if the socket closed due to an error.
- To avoid problems regarding which waiter should remove the pollable from the interest list, the poller must remove the pollable itself.
- If possible, the exclusivity defined in the events mask when informing of the EPOLLHUP should be respected. If not possible or favorable, informing at least one waiter is a must.
### Escrita Tese
- Falar no percurso até encontrar este mecanismo.
	- 1º tentei desenvolver um mecanismo destes com o intuito de servir apenas os links dos sockets, para se poder fazer `waitLink(sid : SocketIdentifier)` e `waitAnyLink() : Socket Identifier`. Na primeira tentativa pensei em criar queue's de pedidos para cada link, e em registar-me em todas as queues, no entanto, surgiram alguns problemas que me levaram a desistir dessa abordagem. O primeiro era não saber como acordar uma thread registada em múltiplos locais sem ser através da criação de uma condição (de lock) criada especialmente para a thread a manifestar interesse em algum link. A criação destas condições a cada pedido não me pareceu razoável, logo ponderei a possibilidade de na API se poder "registar" a thread, ficando uma condição associada e que mais tarde poderia ser eliminada quando o utilizador decidisse que a thread não seria mais utilizada para esse propósito. Para além desse problema, pensei que quando fosse demonstrado o interesse em todos os links (`waitAnyLink()`), registar a thread em todas as queues seria muito pouco eficiente, além de ser necessário posteriormente remover todos esses registos após receber a resposta. Dados esses problemas, e não me apercebendo da utilidade de esperar por uma combinação específica de links para o desenvolvimento de sockets, que por consequência indica uma baixa probabilidade de alguém usar tal funcionalidade, decidi optar por uma outra abordagem.
	- Falar da abordagem que desenvolvi por inteiro, como uma queue global para pedidos que tivessem interesse em todos os links. Falar também que era um pouco rígido demais, consumia créditos para garantir reservas, etc.
	- Depois, surgindo a necessidade de um poller, aqui não se poderia escapar à demonstração de interesse em combinações distintas de elementos, logo decidi explorar os mecanismos mais conhecidos para estas tarefas: select(), poll() e epoll(). De alguma forma tenho de incluir o que descobri sobre estes, e as minhas conclusões sobre as suas utilidades.
	- Falar então que decidi implementar uma versão mais simples do epoll(), que não permite "nesting", e que essencialmente tem como base a ideia que tive inicialmente e descartei. Sendo esta uma versão bem mais sofisticada para além de ser genérica e muito menos rígida que a minha. O formato de registar callbacks que até lá me pareceu perigoso, passou a parecer-me extremamente útil desde que o acesso fosse reservado a partes internas do middleware já que estas, se forem bem testadas não exibirão comportamento malicioso e contraprodutivo. O facto de o interesse ser registado e mantido, até ser explicitamente eliminado, seja pelo callback de espera ou quando já não houver interesse, resolveu uma das minhas preocupações que era a cada chamada registar o interesse em cada pedido só para em breve o ter de remover. A possibilidade de usar em modo level-trigger ou edge-trigger, poder registar-se o interesse como exclusivo ou não, e ainda poder remover o interesse momentaneamente (EPOLLONESHOT) fazem este mecanismo bastante desejável.
	- Relativamente ao problema referido inicialmente de não conhecer uma solução para além de criar uma condição, decidi optar por uma solução que não escapa da criação de um objeto, chama-se ParkState e inclui a referência da thread associada a este e uma variável booleana atómica para determinar quando a thread está à espera e quando é acordada. Apesar de exigir a criação de um objeto, ao combinar o facto de poder manter o registo do interesse, logo não sendo necessário criar o objeto múltiplas vezes, com o facto de não exigir a existência e a aquisição de um lock, esta solução pareceu-me satisfatória. Para uma thread esperar e posteriormente ser acordada, recorri ao mecanismo LockSupport que vim a descobrir posteriormente.
## Reclaim socket's integer ids
- How are file descriptors created? Can they be reclaimed? If so, how is it done?




# Nao preciso de estudar mais o código do epoll. Acho que posso começar a pensar em usar como aplicar algo como o epoll tanto para os links como para os sockets.
- Cada objeto de interesse deve implementar um método de poll() cobrindo os eventos que disponibiliza. Eventos não disponibilizados devem retornar sempre 
- Deve ter uma wait queue que permite adicionar diferentes tipos de entradas conforme o tipo de waiter. Diferentes tipos de entradas podem possuir diferentes estruturas privadas e diferentes métodos de "sinalização". Isto permite implementar a funcionalidade de poll() a um objeto específico para além da funcionalidade do epoll().
## epoll() para os links
- seria possível criar uma instância de epoll() com todos os links como objetos de interesse para se aguardar por a disponibilidade de um link qualquer;
## epoll() para os sockets


# **Estava nos 53:00 da gravacao.**