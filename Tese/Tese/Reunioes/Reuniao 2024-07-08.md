# Conclusões
## Flag "throwIfNone"
- A flag `throwIfNone` quando definida num método, tem como objetivo a emissão de uma exceção na ausência de links estabelecidos e por estabelecer. Isto permite que peças (dinâmicas) da topologia, responsáveis por iniciar processos de estabelecimento de links e que nunca receberão tais pedidos, não fiquem eternamente bloqueadas. 
- Como os sockets são thread-safe, a situação de falta de links, poderia ser resolvida por uma outra thread, no entanto, por conveniência e para facilitar a programação, pensou-se no uso desta flag. 
- Este comportamento é invocado por uma flag já que uma peça fixa da topologia, como um servidor, aguarda que outras peças iniciem o processo de estabelecimento de link, sendo então desejável que este tipo de peças possa ficar bloqueada até que um link seja estabelecido e que não seja emitida uma exceção na ausência de links.
- A flag é primariamente empregada pelos métodos genéricos `waitForLink()` e `waitForAvailableLink()` que esperam pelo estabelecimento de um link ou pela disponibilidade para envio de um link estabelecido, respetivamente.
 - Esta flag pode ser utilizada também em métodos de envio e receção.
	 - O `AbstractSocket` pode ter uma implementação base dos métodos de envio e receção com o argumento `throwIfNone`. Estas implementações devem emitir a exceção `UnsupportedOperationException`. Se for decidido que se pretende implementar esse método, então faz-se o @Override e expõe-se esses métodos na classe *wrapper* do Socket.
## Link retries
### Solução 1
- Permitir criar sockets antes de iniciar a instância. Ao invocar `start()`, a instância passa a processar mensagens recebidas que incluem pedidos de ligação. Deste modo, evita-se rejeitar pedidos de ligação porque a criação do socket está atrasada.
	```
		// create middleware instance
		A3MMiddleware middleware = A3MMiddleware.new("node1","192.168.1.99", 12345); 
		// create socket
		RepSocket server = RepSocket.create("server", middleware);
		// starts middleware
		middleware.start();
	```
- O problema desta solução é não cobrir sockets que são criados após a instância ser iniciada.
### Solução 2 (final)
- Inclui a proposta da solução 1.
- Contempla retries de pedidos de ligação.
- O ZeroMQ não parece ter limites de pedidos de ligação, mas não custa incluir como uma opção base que tem como valor default "infinito". Possui, no entanto, um mínimo e máximo para o timeout entre cada pedido. A cada falha no pedido, o valor do timeout aumenta exponencialmente até ao valor máximo se este tiver sido definido. Se o valor máximo não tiver sido definido, o valor de timeout permanece igual.
	- Não deve ser necessário valores de timeouts aleatórios para proteger contra thundering herd, mas pode ser algo a mencionar na tese.
- As novas tentativas de ligação devem ser executadas pela única thread do middleware (a reader thread). Para isto é necessário que os sockets tenham um método privado, para não ser exposto às subclasses, que aquando da receção de uma resposta negativa a um pedido de ligação por ausência do socket destino, permite o socket criar um evento `LinkRetryEvent` e colocá-lo na queue de eventos a serem realizados pela reader thread.
	- Um evento do tipo `SendMsgEvent` que permitiria agendar o envio de uma mensagem parece uma solução válida e útil até para outras situações, no entanto, é importante que antes de reenviar o pedido de ligação se verifique que o socket não se encontra fechado e que o cliente não cancelou o processo de ligação com o destino em questão (através do método `unlink()`).
	- Para realizar estas verificações, a reader thread, no momento de execução do evento `LinkRetryEvent` apenas necessita de executar o método `linkRetry(sid : SocketIdentifier)` do socket que se encontra incluído no objeto do evento.
	- O método `linkRetry(sid : SocketIdentifier)` fará todas as verificações necessárias: (1) socket não se encontra fechado; (2) link em questão encontra-se num estado de aguardar retry.
	- Se o método tiver visibilidade "default" pode evitar-se que seja visto por classes que não devem, mas não deixa de ser poder ser exposto se uma subclasse do `AbstractSocket` for criada dentro do meu package.
## Expor eventos do socket
- A exposição de eventos do socket como "Link estabelecido com X", "Link recusado por X", "Sem links", etc.
- No caso de ser necessário e mesmo relevante a exposição destes eventos, estes podem ser expostos a partir de um método diferente do `receive()` de mensagens. Isto permite evitar que a programação do fluxo normal seja complicada desnecessariamente. 
- No fundo, isto poderia ser um *log* do socket.
- Neste momento, esta funcionalidade não aparenta ser necessária. ZeroMQ e NNG não possuem nada parecido com esta funcionalidade.

## Wait for link availability (to send)

Ensuring a link is available for sending a message is crucial for developing blocking send methods across various socket types. While some socket implementations can use a generic send method that includes the waiting functionality, others require a method that exclusively waits for sending availability, allowing the socket to decide if and what message should be sent.

Therefore, in addition to the generic blocking send method, two key functionalities are needed: waiting for a specific link to become available (`waitForLinkAvailability(sid: SocketIdentifier)`) and waiting for any link to become available (`waitForAvailableLink()`).

<!-- The "waiting" capability will be provided by a flow control coordinator, while the actual credits of each link will be managed by a flow control manager. This enables the separation of the concerns of coordinating threads and managing the flow control. In this section, we will be talking about different implementations of flow control coordinators and how a flow control coordinator is employed alongside a flow control manager. -->  

### Waiting for a Specific Link

Waiting for a specific link is a straightforward problem, commonly referred to as the Producer-Consumer problem. This can be easily solved using a single lock condition. When the link has credits, threads can return immediately. If the balance is insufficient, threads must wait using the lock condition. When credits are replenished, the lock condition is used to signal the appropriate number of threads that can be satisfied.

### Waiting for Any Link

The complexity increases when waiting for any link to become available. The main challenges are related to fairness, starvation and the thundering herd problem.

1. **Fairness/Starvation**: The primary issue is avoiding the prioritization of link-specific[^1] or general threads[^2].  Evenly distributing link credits between waiting threads and preventing threads from never being signaled depends on the fairness of the signaling mechanism chosen by the client.

2. **Thundering Herd**: This problem is addressed by avoiding signaling threads that cannot perform work, thereby preventing unnecessary context switches that degrade performance. Signaling a thread when the return condition cannot be met results in two unnecessary context switches since the thread needs to wait again.

[^1] Link-specific threads are threads interested in a single link.
[^2] General threads are threads interested in any link.

### Basic Implementation

A basic implementation might use a lock condition per link for link-specific waiting operations and a general lock condition for any link waiting operations. When a link's credits are replenished, the `signalAll()` method of both conditions could be invoked. Signaled threads would then check the link balance and return if is positive; if not, they would wait again. This approach, however, is not ideal due to potential races for credits, increased lock contention, pointless context switches, and starvation of slower threads.

### Advanced Implementations

The key to address the mentioned challenges is to use requests. There are two types of requests: link-specific and general. With each request corresponding to a thread's intention to wait for sending availability, the thundering herd problem is solved by only signaling the amount of threads that can be satisfied by the credit balance that each link allows. Regarding the starvation and fairness problems, two solutions are provided for the client to choose from: a *fair-ish* version and an unfair version. 

### Fair-ish implementation

To address the fairness and starvation problems, the devised solution involves requests that offer a sense of order. This order is achieved by giving to each request a ticket number. Each request is then placed in an appropriate queue depending on its type. Each link has its own queue of link-specific requests, and general requests are kept in a common queue. When the credits of a link are increased, it signals waiting threads based on the ticket numbers of the requests. The link peeks at the first ticket number of both its own link-specific requests queue and the general requests queue. The queue with the smallest ticket number (or with requests) dictates which type of waiting thread should be signaled. This approach solves the fairness/starvation problem between different types of threads. To ensure fairness among threads of the same type, a fair version of the lock is employed.

To prevent credits from being stolen, requests have three states: new, unsettled, and settled. A request is new when created, becomes unsettled when a thread is signaled, and becomes settled (deleted) when a thread reacquires the lock and secures the credit. The stealing action is effectively prevented by only allowing "waiting" threads to secure a credit when there are requests, of its type, that can be settled. New "waiting" threads can secure a credit immediately if there are enough link credits to cover both the new and unsettled requests, as well as the current request. While allowing this immediate action is not completely fair[^3], for efficiency purposes this solution seemed acceptable as it prevents two unnecessary context switches: one when threads enter the waiting queue; and another when they are signaled shortly after.

[^3] It is not entirely fair because a new thread is allowed to consume a credit before the queued threads.

<span style="color:red">Não esquecer de emitir exceção quando deixa de existir links. Semelhante ao processo feito para quando um link específico é eliminado.</span>
### Unfair Implementation

The unfair implementation is much more relaxed on the fairness and starvation problems. Apart from interleaving (when possible[^4]) signals of link-specific and general threads, there is no attempt in signaling threads in the order in which the requests were issued. Furthermore, credit stealing is not prevented. A thread that in the fair-ish version would be queued, due to the insufficient amount of credits to satisfy both the thread in question and the threads already waiting, can consume a credit instantaneously.

[^4] Interleaving is only possible when there are requests of both types.
<span style="color:red">Faz sentido fazer interleaving por link especifico ou faria mais sentido fazer interleaving a nível geral? Isto porque </span>
<span style="color:red">Usar algoritmo de sinalizacao em funcao do nº de threads a sinalizar? Na fair version nao da por causa da ordem.</span>
### Credit reservation

The issues surrounding "waiting for availability" are not over. 

The first problem is that a thread that has successfully returned from its "waiting" method invocation should be able to send a message to the link that became available whenever it chooses to. For this to be feasible, credits must be reserved. This reservation is achieved by preemptively consuming the credit, as the advanced implementations' do before the methods return. Without reserved credits, a thread that does not consume the credit immediately after returning from the waiting method[^5] might have its credit stolen by another thread[^6].

The second problem is related to the preemptive consumption of credits. A thread may decide not to send a message after waiting for availability. In such cases, a `releaseCredit()` method must be used to "release" the credit and to signal another thread. Releasing credits cannot be done simply by increasing the credits of the link, as this could lead to credit farming. To prevent farming, settled requests should be temporarily saved instead of being immediately deleted. A settled request should only be deleted when a thread either acquires or releases a credit. By making the `releaseCredit()` fail when there are no settled requests, credit farming is prevented. Credit acquisition, which acts more as a confirmation, is done by invoking `acquireCredit()`, which can also fail[^7] due to the lack of settled requests.

[^5] Until the socket lock is unlocked or the thread enters a waiting state using a lock condition.
[^6] Positive credits resulting from an increase in credit balance signal new threads. If a credit is not reserved, a new thread could be signaled, leading to a race for that credit. Additionally, a new "waiting" thread could acquire the credit if the necessary conditions are met.
[^7] The failure of the `acquireCredit()` method is not relevant, as the consumption of credits is done by the "wait...()" methods.

<p style="color:red">Usar um coordenador que nao pode atualizar diretamente os créditos pode resultar na falha de um envio pq creditos foram reservados no coordenador mas entretanto houve reducao de creditos que passou o nº de creditos efetivo do link para negativo ou mesmo 0. Com o coordenador desligado entao pode nao fazer sentido aceitar que as mensagens sejam enviadas já que os créditos efetivos nao foram alterados para manifestar as reservas dos creditos.  No entanto, se estiver ligado, a reserva é feita atraves do consumo do credito, indicando que a mensagem é como se tivesse sido efetivamente enviada, permitindo que as mensagens sejam enviadas imediatamente caso existam creditos por "adquirir".</p>

### Exposing the functionality

After discussing the implementation of the "waiting for availability" functionality, we will now address how this can be presented to developers who wish to implement a custom socket. The following methods are implemented by an abstract socket class as final protected methods:
- `waitForLinkAvailability(sid: SocketIdentifier)` and `waitForAvailableLink(): SocketIdentifier` are exposed as they are.
- `releaseSendCredit()` simply renames the `releaseCredit()` method.
- `sendDataMsg(message: Msg, reservedCredit: boolean)`, used to send messages. This method allows indicating if a credit has been reserved, thus enabling the waiting process to be skipped. The `acquireCredit()` method is not exposed directly since credit acquisition should only occur when a message is actually being sent.
- `sendDataMsg(message: Msg)`, which assumes that a credit has not been reserved, and thus, will block if necessary.
- `trySendDataMsg(message: Msg): boolean`, which attempts immediate sends. If the operation would normally block, the message is not sent, and "false" is returned. "True" is returned when the data message is successfully sent.

The send methods all include the word "data" to indicate that only data messages are limited by credits. Control messages bypass the flow control mechanism to avoid blocking their send, which could potentially lead to deadlocks.

Additionally, all methods that can block have variants allowing the specification of a timeout.

Custom sockets may further expose the waiting functionality to clients if it is deemed useful due to the socket's semantics.

<span style="color:red">Após pensar mais um pouco, separar o coordenador de espera dos créditos efetivos pode ser mesmo necessário. Uma thread que reserva vários créditos sobre um ou múltiplos links não deve poder enviar as mensagens quando lhe apetecer. Após reservar o crédito estou essencialmente a permitir que a thread demore o tempo que quiser e envie a mensagem sem qualquer restrição. Isto viola a possibilidade do recetor impedir o envio de mensagens já que existem envios "reservados", apesar de que num contexto exactly-once não devem existir restrições temporais.</span>
## Pollers

### Documentation
https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/?h=v6.10
https://lxr.linux.no/linux+v6.7.1/fs/select.c
https://man7.org/linux/man-pages/man2/select.2.html
https://man7.org/linux/man-pages/man2/poll.2.html
https://man7.org/linux/man-pages/man7/epoll.7.html
https://man.freebsd.org/cgi/man.cgi?kqueue
#### select()
##### Arguments
- A file descriptor set is passed for each type of event of interest: `readfds` (can read), `writefds` (can write), `exceptfds` (exceptional conditions). NULL can be passed if that type of event is not of interest.
- Another important argument is `nfds` which should be set to the highest file descriptor present in any of the three sets.  
- The file descriptor sets are interpreted as bit arrays, with each bit corresponding to a file descriptor. A bit as 1 conveys the interest in the event for that file descriptor.
- Each bit array (file descriptor set) needs to have a bit for each file descriptor until the highest mentioned file descriptor. For example, if the highest file descriptor we have interest in is 5705, then the sets need to have at least 5705 + 1 = 5706 bits (+1 because file descriptors start at 0). To prevent unaligned accesses, the size of the sets is actually calculated in terms of unsigned longs, i.e., the size should be a multiple of 64 bits. So the actual size would be 5760 bits = 90 unsigned longs.     
##### Underlying Data Structures
The underlying data structures used, which are also used by the poll() system call, are:
###### `struct poll_wqueues` 
- Structure: 
```
struct poll_wqueues {
	// initialized by "init_poll_funcptr(&pwq->pt, __pollwait)"
	// Holds the call that creates a wait queue entry and effectively
	// queues the current task to the be signaled by the file descriptors of interest.
	// Also holds the key, i.e., the events of interest
	poll_table pt; 

	// When the size of the inline poll table entries are not enough,
	// more space for poll table entries are allocated
	// in the format of poll table pages. 
	struct poll_table_page *table = NULL;

	// identification of the current task, which is required to wake
	// up the task when an event is ready
	struct task_struct *polling_task = current;   

	// set to 1 when a wait queue entry is processed
	int triggered = 0;  

	// if any error occurs, this is set
	int error = 0; 
	
	// index of the next inline entry to be created
	int inline_index = 0;  
	// faster access poll table entries
	struct poll_table_entry inline_entries[N_INLINE_POLL_ENTRIES]; 
};
```
- It is a structure used to hold all poll wait entries, is used to inform when a file descriptor of interest is ready (through the `triggered` variable) or when an error occurs, and also to hold the identification of the polling task (required to wake it up when an event is ready). 
- Holding the poll wait entries is required to enable wait entries to be deleted before `select()` returns.

###### `poll table` 
- Structure:
```
// Poll table
typedef struct poll_table_struct {
	poll_queue_proc _qproc; // function to queue the process in a poll queue
	__poll_t _key = ~(__poll_t)0; // events of interest. Initialized with all events enabled.
} poll_table;
```
- Essentially used to hold the function that queues the process when an immediate poll is not possible and also the key which indicates the events of interest.
- initialized by `init_poll_funcptr(&pwq->pt, __pollwait)`, which sets the call required to create wait queue entries and sets the key (all events are enable by default).
- The `_qproc` function is essential to queue the process in the queues of the file descriptors of interest, enabling the process to be signaled when an event of interest is ready.

###### `struct poll_table_entry`
- Structure:
```
struct poll_table_entry {
	struct file *filp;
	__poll_t key;
	wait_queue_entry_t wait;
	wait_queue_head_t *wait_address;
};
```
- This structures records the interest in a file descriptor. 
- The `filp` variable is a pointer to the information of the "file" of interest.
- The `key` variable informs the events of interest for that file.
- The `wait` variable is the pointer to the wait queue entry registered in the file's queue(s). (I think the file's have a queue for each type of event). This is required to identify to remove the entry, which was not processed, before the system call returns.
- The `wait_address` variable is the head of the queue where the wait queue entry was queued. This is required to delete entries before the select() system call returns.

###### `struct poll_table_page`
- Structure:
```
struct poll_table_page {
	struct poll_table_page * next;
	struct poll_table_entry * entry;
	struct poll_table_entry entries[];
};
```
- As mentioned before, this is nothing more than a structure used to allocate space for multiple poll table entries, when the inline entries array is already full and when the current page is full.
- 
###### `struct wait_queue_entry`
- Structure:
```
struct wait_queue_entry {
	unsigned int		flags;
	void			*private;
	wait_queue_func_t	func; // wake up function (pollwake)
	struct list_head	entry;
} wait_queue_entry_t;
```
- Wait queue entry which is used to register the interest of a process in waking up when an file is ready.
- The `private` variable holds the pointer to the `struct poll_wqueues` that defines the polling task, and is required to wake up the process.
- The `func` variable holds the wake up function which uses the `private` variable to do so.
###### `struct wait_queue_head`
- Structure:
```
struct wait_queue_head {
	spinlock_t		lock;
	struct list_head	head;
} wait_queue_head_t;
```
- Pointer to the entry that is the head of the queue. Required to keep track of the head of the queue, and allow entries to be removed when they are no longer required. An example, is the clean up of wait entries before the select() method returns. The clean up is required since each select() call is individual, i.e., is not related to any other select() call.
##### Algorithm
- The select() call iterates over all the file descriptors present in the file descriptor sets, and checks if there is any file ready for a specified event of interest.
- If a timeout of 0 is given, then creating wait entries is unnecessary, as they would be deleted right after.
- If a timeout higher than 0 is given, for each file descriptor which can block and has an event of interest, a wait entry is created.
- After iterating over all file descriptors, if a timeout was set and expired or a file descriptor is ready for an event of interest, the call exits the loop where the iteration through the file descriptors is done. It then proceeds to remove any existent wait entries, before returning.
- If after iterating over all file descriptors, there isn't a file descriptor ready and the timeout (if set) has not expired, the process sleeps until the timeout expires or it is signaled or a spurious awake happens. Upon waking up, it repeats the process of iterating over the file descriptors, and effectively exits the loop if the timeout expired. 
##### Notes
- select() can monitor only file descriptors numbers that
are less than FD_SETSIZE (1024)
#### poll()
- Very similar to select(). The difference lays in the structures used to pass the file descriptors and events of interest, as well as the structures used to iterate over them.
- Does not have a restriction regarding the file descriptors as the select() does.
- Instead of using different file descriptor sets for each of the events (read, write and exceptional), only a set of poll file descriptors are provided (the structure can be seen below). The poll file descriptors contain the file descriptor of an open file, the events of interest, and the variable to inform the events of interest which are ready.
- Structure of poll file descriptor:
```
struct pollfd {
	int fd;
	short events;
	short revents;
};
```
#### select()/poll() questions
1. **The "file->f_op->poll" function is the one responsible for creating and queuing the current task, right?**
    - Yes, the `file->f_op->poll` function is indeed responsible for checking the current status of the file descriptor and for queuing the current task if necessary. When `select()` or `poll()` is called, it will iterate over the file descriptors, invoking their `poll` method (via `file->f_op->poll`). This `poll` method typically calls functions like `poll_wait()` to add the current task to the wait queue associated with the file descriptor if it cannot immediately determine that the file descriptor is ready for the requested I/O operations.
    
2. **A socket has a wait queue for each type of event, or has a common queue for all events?**
    - A socket typically maintains separate wait queues for different types of events. For instance, there are distinct wait queues for reading, writing, and exceptional conditions. This allows the socket to wake up tasks waiting for a specific type of event without affecting tasks waiting for other types of events. This separation is beneficial because it provides finer granularity in event notification and improves efficiency.

3. **When an event is ready, the "file" iterates over a wait queue and wakes up the tasks that are interested in the file. When waking up, are the wait queue entries removed? Or, are the wait queue entries removed only before freeing the struct poll_wqueues?**
    - When an event is ready, the `wake_up` functions are used to wake up tasks waiting on the wait queue. Typically, when a task is woken up, its wait queue entry is removed. This is because the task no longer needs to wait for that event. However, in the context of `poll()` and `select()`, the wait queue entries are also removed when the polling operation completes, i.e., before the `poll_wqueues` structure is freed. This ensures that all wait queue entries associated with the polling operation are cleaned up properly to avoid dangling pointers or memory leaks.

In summary:

- `file->f_op->poll` handles the queuing of the current task.
- Sockets maintain separate wait queues for different event types.
- Wait queue entries are typically removed when the task is woken up, and they are also explicitly removed before freeing the `poll_wqueues` structure to ensure proper cleanup.
#### epoll()
- 


#### kqueue()


#### \[Only for code studying purposes, not for thesis\] Understanding select() / poll() underlying data structures


##### poll waiting queues
```
/*
 * Structures and helpers for select/poll syscall
 */
struct poll_wqueues {
	poll_table pt; // initialized by "init_poll_funcptr(&pwq->pt, __pollwait)"
	struct poll_table_page *table = NULL;
	struct task_struct *polling_task = current;
	int triggered = 0;
	int error = 0;
	int inline_index = 0;
	struct poll_table_entry inline_entries[N_INLINE_POLL_ENTRIES];
};
```

##### poll table
```
// Poll table
typedef struct poll_table_struct {
	poll_queue_proc _qproc = *1; // function to queue the process in a poll queue
	__poll_t _key = ~(__poll_t)0; // events of interest. Initialized with all events enabled.
} poll_table;
```

```
------------------>  *1  <------------------

// _qproc initialized value
static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
				poll_table *p)
{
	// calculates pointer of struct poll_wqueues which owns the poll_table
	struct poll_wqueues *pwq = container_of(p, struct poll_wqueues, pt);
	
	// gets pointer to next poll table entry. Creates a table page if required.
	struct poll_table_entry *entry = poll_get_entry(pwq);
	if (!entry)
		return;
	
	// fills entry with file descriptor, wait address and key (events enabled)	
	entry->filp = get_file(filp);
	entry->wait_address = wait_address;
	entry->key = p->_key;

	// initializes wait queue entry with poll wake up function
	init_waitqueue_func_entry(&entry->wait, pollwake);

	// ?? Ainda nao sei para que é preciso o identificador da pwq
	entry->wait.private = pwq;
	
	// adds wait queue entry to the queue
	add_wait_queue(wait_address, &entry->wait);
}
```

```
// format of a function to queue a process on poll queue? 
typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *, struct poll_table_struct *);

// In this function, we can see that the poll_table, "p", adds itself to the poll queue 
static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)
{
	if (p && p->_qproc && wait_address)
		p->_qproc(filp, wait_address, p);
}
```

##### poll table page

```
struct poll_table_page {
	struct poll_table_page * next;
	struct poll_table_entry * entry;
	struct poll_table_entry entries[];
};
```
##### poll table entry
```

struct poll_table_entry {
	struct file *filp;
	__poll_t key;
	wait_queue_entry_t wait;
	wait_queue_head_t *wait_address;
};
```

##### wait queue entry
```
struct wait_queue_entry {
	unsigned int		flags;
	void			*private;
	wait_queue_func_t	func; // wake up function (pollwake)
	struct list_head	entry;
} wait_queue_entry_t;
```

```
struct wait_queue_head {
	spinlock_t		lock;
	struct list_head	head;
} wait_queue_head_t;
```

```
static int pollwake(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
{
	struct poll_table_entry *entry;

	// gets poll table entry that owns the wait queue entry
	entry = container_of(wait, struct poll_table_entry, wait);
	
	//
	if (key && !(key_to_poll(key) & entry->key))
		return 0;
	return __pollwake(wait, mode, sync, key);
}
```


### Problems
- How can it work in conjunction with the "waiting for availability" mechanism?
- Level-triggered, edge-triggered, mixed, custom?
- Return one available socket or all sockets?
- Thread-safe or not?
- Should it reserve credits as well? Maybe not.
- How do select() and poll() know which objects are ready, and how does the kernel know which process to wake up.
- Can the "wait for availability coordinator" be somehow used to implement a thread-safe poller? Each socket would essentially have a send poller with all links.
- Allowing events, exceptions to be subscribed can also be beneficial.
- Some sockets are not compatible with reading or sending. How to handle this situations? Return always true? Return always false?
	- False should be the answer, as true would result in the return of the method.


## Reclaim socket's integer ids
- How are file descriptors created? Can they be reclaimed? If so, how is it done?






- Estava nos 51:00 da gravacao.